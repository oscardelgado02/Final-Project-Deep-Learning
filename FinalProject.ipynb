{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2470,"status":"ok","timestamp":1686759910611,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"YYx2zhqDCOPq","outputId":"f89ae6aa-d3ed-4508-b022-570258b0fc3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# MOUNT FOR GOOGLE DRIVE\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5056,"status":"ok","timestamp":1686759915665,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"Hn07EATxC7AM","outputId":"b963619b-3860-42b4-ea98-11d922f6352b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ultralytics\n","  Downloading ultralytics-8.0.117-py3-none-any.whl (599 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.6/599.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (8.4.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n","Installing collected packages: ultralytics\n","Successfully installed ultralytics-8.0.117\n"]}],"source":["!pip install ultralytics\n","# !git clone https://github.com/ultralytics/yolov5\n","# %cd yolov5\n","# !pip install -r requirements.txt\n","\n","# %cd /content/drive/MyDrive/FinalProjectDeepLearning"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686759915665,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"flR5UtmErIWW"},"outputs":[],"source":["# REFERENCES\n","\n","# GITHUB yolo https://github.com/ultralytics/yolov5"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2706,"status":"ok","timestamp":1686759918368,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"mZmYkY0UC8bJ"},"outputs":[],"source":["# IMPORTS\n","import os\n","import torch\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import cv2\n","import itertools\n","import pandas as pd\n","from ultralytics import YOLO\n","import xml.etree.ElementTree as ET\n","import shutil\n","import random\n","from google.colab.patches import cv2_imshow\n","import glob\n","\n","# Change directory to \"FinalProjectDeepLearning\"\n","os.chdir('/content/drive/MyDrive/FinalProjectDeepLearning')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1686759918368,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"_L21PxE4DKE1"},"outputs":[],"source":["# DATA PATHS\n","data_path = '/content/drive/MyDrive/FinalProjectDeepLearning/Dataset'\n","full_ijcnn_path = '/content/drive/MyDrive/FinalProjectDeepLearning/FullIJCNN2013'\n","test_videos_path = '/content/drive/MyDrive/FinalProjectDeepLearning/TestVideos'"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686759918368,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"RFUw4pelETxU"},"outputs":[],"source":["# CLASS LABELS\n","trafficlight = 0\n","speedlimit = 1\n","crosswalk = 2\n","stop = 3"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":812,"status":"ok","timestamp":1686759919178,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"NBrXB50JEEtM"},"outputs":[],"source":["## DEFINITIONS\n","\n","# FULLIJCNN DATA CONVERSION\n","\n","def convert_ppm_to_png(ppm_path, png_path):\n","    image = cv2.imread(ppm_path + '.ppm', cv2.IMREAD_UNCHANGED)\n","    cv2.imwrite(png_path + '.png', image)\n","\n","def get_image_dimensions(image):\n","    height, width = image.shape[:2]\n","    return width, height\n","\n","def create_text_file(file_path, line_content):\n","    with open(file_path+'.txt', 'w') as file:\n","        file.write(line_content)\n","\n","def convert_IJCNN_to_dataset():\n","  full_ijcnn_labels_to_speedlimit = [0,1,2,3,4,5,7,8]\n","  full_ijcnn_labels_to_stop = 14\n","\n","  with open(full_ijcnn_path + '/gt.txt', 'r') as file_:\n","      lines_full_ijcnn  = file_.readlines()\n","\n","  for line in lines_full_ijcnn:\n","    splited_line = line.split(';')\n","\n","    file_name = splited_line[0][:-4]\n","    left_col = int(splited_line[1])\n","    top_row = int(splited_line[2])\n","    right_col = int(splited_line[3])\n","    bottom_row = int(splited_line[4])\n","    label = int(splited_line[5])\n","\n","    if label == full_ijcnn_labels_to_stop:\n","      label = stop\n","    elif label in full_ijcnn_labels_to_speedlimit:\n","      label = speedlimit\n","    else:\n","      label = -1\n","\n","    if label >= 0:\n","      image = cv2.imread(full_ijcnn_path + '/' + file_name + '.ppm')\n","      width, height = get_image_dimensions(image)\n","\n","      bounding_box = convert_boundary((width, height), (left_col, right_col, top_row, bottom_row))\n","\n","      with open(f'{data_path}/labels/{file_name}.txt', 'a') as out_file:\n","        out_file.write(\" \".join([str(a) for a in (label, *bounding_box)]) + '\\n')\n","\n","      convert_ppm_to_png(full_ijcnn_path + '/' + file_name, data_path + '/images/' + file_name)\n","\n","# FIRST DATASET DATA CONVERSION\n","\n","def convert_boundary(size, boundary):\n","    dwidht, dheight = 1. / size[0], 1. / size[1]\n","    widht = boundary[1] - boundary[0]\n","    height = boundary[3] - boundary[2]\n","    x = (boundary[0] + boundary[1]) / 2.0 - 1\n","    y = (boundary[2] + boundary[3]) / 2.0 - 1\n","    return x * dwidht, y * dheight, widht * dwidht, height * dheight\n","\n","def convert_RSD_to_dataset():\n","    for file_ in os.listdir(data_path+'/labels'):\n","        if file_.endswith('.xml'):\n","            file_name = file_.split('.')[0]\n","            out_file = open(data_path + '/labels/' + file_name + '.txt', 'w')\n","\n","            tree = ET.parse(data_path + '/labels/' + file_)  # for the xml\n","            root = tree.getroot()\n","            size = root.find('size')\n","            width = int(size.find('width').text)\n","            height = int(size.find('height').text)\n","\n","            labels = ['trafficlight', 'speedlimit', 'crosswalk', 'stop']\n","\n","            for obj in root.iter('object'):\n","                class_name = obj.find('name').text\n","                if class_name in labels and int(obj.find('difficult').text) != 1:\n","                    xmlbox = obj.find('bndbox')\n","                    bounding_box = convert_boundary((width, height), [float(xmlbox.find(x).text) for x in ('xmin', 'xmax', 'ymin', 'ymax')])\n","                    class_id = labels.index(class_name)\n","                    out_file.write(\" \".join([str(a) for a in (class_id, *bounding_box)]) + '\\n'))\n","\n","\n","# ELIMINAR IMAGENES EN PARTICION DATASET\n","\n","def clear_folder(folder_path):\n","  for filename in os.listdir(folder_path):\n","    file_path = os.path.join(folder_path, filename)\n","    if os.path.isfile(file_path):\n","      os.remove(file_path)\n","    elif os.path.isdir(file_path):\n","      clear_folder(file_path)\n","      os.rmdir(file_path)\n","\n","# DATA DIVISIONS\n","\n","def contains_string(target_string, string_list):\n","  for string in string_list:\n","    if string in target_string:\n","      return True\n","  return False\n","\n","def data_division():\n","  imagesForDA = os.listdir(data_path + 'images')\n","  labelsForDA = os.listdir(data_path + 'labels')\n","  list_of_validation_samples = []\n","  list_of_test_samples = []\n","\n","  for fileNameImage in imagesForDA:\n","    if contains_string(fileNameImage, list_of_validation_samples):\n","      shutil.move(data_path + 'images/'+fileNameImage, data_path + 'validation/images/'+fileNameImage)\n","    elif contains_string(fileNameImage, list_of_validation_samples):\n","      shutil.move(data_path + 'images/'+fileNameImage, data_path + 'test/images/'+fileNameImage)\n","    else:\n","      shutil.move(data_path + 'images/'+fileNameImage, data_path + 'train/images/'+fileNameImage)\n","\n","  for labelName in labelsForDA:\n","    if labelName.endswith(\".txt\"):\n","      if contains_string(labelName, list_of_validation_samples):\n","        shutil.move(data_path + 'labels/'+labelName, data_path + 'validation/labels/'+labelName)\n","      elif contains_string(labelName, list_of_validation_samples):\n","        shutil.move(data_path + 'labels/'+labelName, data_path + 'validation/labels/'+labelName)\n","      else:\n","        shutil.move(data_path + 'labels/'+labelName, data_path + 'test/labels/'+labelName)\n","\n","# DEVOLVER IMAGENES A ORIGINAL\n","\n","def return_to_original():\n","  train_images = os.listdir(data_path + '/train/images')\n","  train_labels = os.listdir(data_path + '/train/labels')\n","\n","  validation_images = os.listdir(data_path + '/validation/images')\n","  validation_labels = os.listdir(data_path + '/validation/labels')\n","\n","  test_images = os.listdir(data_path + '/test/images')\n","  test_labels = os.listdir(data_path + '/test/labels')\n","\n","  for fileNameImage in train_images:\n","    shutil.move(data_path + '/train/images/'+fileNameImage, data_path + '/images/'+fileNameImage)\n","\n","  for fileNameImage in validation_images:\n","    shutil.move(data_path + '/validation/images/'+fileNameImage, data_path + '/images/'+fileNameImage)\n","\n","  for fileNameImage in test_images:\n","    shutil.move(data_path + '/test/images/'+fileNameImage, data_path + '/images/'+fileNameImage)\n","\n","  for fileLabel in train_labels:\n","    shutil.move(data_path + '/train/labels/'+fileLabel, data_path + '/labels/'+fileLabel)\n","\n","  for fileLabel in validation_labels:\n","    shutil.move(data_path + '/validation/labels/'+fileLabel, data_path + '/labels/'+fileLabel)\n","\n","  for fileLabel in test_labels:\n","    shutil.move(data_path + '/test/labels/'+fileLabel, data_path + '/labels/'+fileLabel)\n","\n","#OTHER WAY TO DO DATA DIVISION\n","\n","def divide_list_random(list_data, percentage):\n","    random.shuffle(list_data)\n","    length = len(list_data)\n","    split_index = int(length * percentage / 100)\n","    list_part1 = list_data[:split_index]\n","    list_part2 = list_data[split_index:]\n","    return list_part1, list_part2\n","\n","def data_division_with_percentage():\n","  imagesForDA = os.listdir(data_path + '/images')\n","  labelsForDA = os.listdir(data_path + '/labels')\n","\n","  labels = []\n","\n","  for label in imagesForDA:\n","    labels.append(label[:-4])\n","\n","  labels_train, labels_validation = divide_list_random(labels, 90)\n","  labels_validation, labels_test = divide_list_random(labels_validation, 50)\n","\n","  for name in labels_train:\n","    shutil.move(data_path + '/images/'+name+'.png', data_path + '/train/images/'+name+'.png')\n","    shutil.move(data_path + '/labels/'+name+'.txt', data_path + '/train/labels/'+name+'.txt')\n","\n","  for name in labels_validation:\n","    shutil.move(data_path + '/images/'+name+'.png', data_path + '/validation/images/'+name+'.png')\n","    shutil.move(data_path + '/labels/'+name+'.txt', data_path + '/validation/labels/'+name+'.txt')\n","\n","  for name in labels_test:\n","    shutil.move(data_path + '/images/'+name+'.png', data_path + '/test/images/'+name+'.png')\n","    shutil.move(data_path + '/labels/'+name+'.txt', data_path + '/test/labels/'+name+'.txt')\n","\n","# Function to delete files with exceptions\n","\n","def delete_exception_files(exception_path, directory):\n","    # Load exceptions from the file\n","    with open(exception_path, 'r') as file:\n","        exceptions = file.read().splitlines()\n","\n","    # Go through each exception\n","    for exception in exceptions:\n","        exception = exception.strip(',') # Remove trailing comma if it exists\n","        # Look for any file that includes the exception in its name, regardless of its extension\n","        for filepath in glob.glob(directory + f'/**/*{exception}.*', recursive=True):\n","            print(f'Deleting: {filepath}')\n","            os.remove(filepath)\n","\n","# Freeze\n","def freeze_layer(trainer):\n","    model = trainer.model\n","    num_freeze = 10\n","    print(f\"Freezing {num_freeze} layers\")\n","    freeze = [f'model.{x}.' for x in range(num_freeze)]  # layers to freeze\n","    for k, v in model.named_parameters():\n","        v.requires_grad = True  # train all layers\n","        if any(x in k for x in freeze):\n","            print(f'freezing {k}')\n","            v.requires_grad = False\n","    print(f\"{num_freeze} layers are freezed.\")\n","\n","# convert_RSD_to_dataset()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1686759919179,"user":{"displayName":"OSCAR DELGADO RUEDA","userId":"08180167425829280723"},"user_tz":-120},"id":"oJio9mrJWbaV","outputId":"c266a08a-9bd6-4709-8af5-da4db3c529a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["%cd /content/drive/MyDrive/FinalProjectDeepLearning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4HQBtObnzns","outputId":"695ffcf8-95e4-404d-e2ea-5e2085c050ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement \"gitpython\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 7.9 MB/s eta 0:00:00\n","Collecting gitdb<5,>=4.0.1 (from gitpython)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 321.5 MB/s eta 0:00:00\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, gitdb, gitpython\n","Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['gitpython']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=TrafficVOC.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=50, batch_size=16, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=yolov5/runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","Command 'git fetch origin' timed out after 5 seconds\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement \"thop>=0.1.1\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop>=0.1.1) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop>=0.1.1) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop>=0.1.1) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop>=0.1.1) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop>=0.1.1) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop>=0.1.1) (1.3.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/yolov5/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 🚀 v7.0-178-ga199480 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 15.8MB/s]\n","Overriding model.yaml nc=80 with nc=4\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     24273  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","Model summary: 214 layers, 7030417 parameters, 7030417 gradients\n","\n","Transferred 343/349 items from yolov5s.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/Dataset/train/labels... 1060 images, 0 backgrounds, 0 corrupt: 100% 1060/1060 [05:02<00:00,  3.51it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/Dataset/train/images/00340.png: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/Dataset/train/labels.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/Dataset/validation/labels... 59 images, 0 backgrounds, 0 corrupt: 100% 59/59 [00:30<00:00,  1.96it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/.shortcut-targets-by-id/1vDOb7TZctC0hjJ7LI48z26TiJSDQRbb8/FinalProjectDeepLearning/Dataset/validation/labels.cache\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.73 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to yolov5/runs/train/exp5/labels.jpg... \n","Image sizes 320 train, 320 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1myolov5/runs/train/exp5\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       0/49       1.1G     0.1058    0.01538    0.03594          9        320: 100% 67/67 [01:01<00:00,  1.09it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:04<00:00,  2.10s/it]\n","                   all         59         84      0.554     0.0484     0.0585     0.0237\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       1/49       1.1G    0.07343    0.01828    0.01995         12        320: 100% 67/67 [00:53<00:00,  1.25it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.81it/s]\n","                   all         59         84      0.823      0.105      0.124     0.0596\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       2/49      1.12G    0.06697    0.01472    0.01431         10        320: 100% 67/67 [00:54<00:00,  1.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.82it/s]\n","                   all         59         84      0.389      0.429      0.355      0.146\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       3/49      1.12G    0.06041    0.01286    0.01007          8        320: 100% 67/67 [00:54<00:00,  1.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.08it/s]\n","                   all         59         84      0.493      0.436      0.393      0.164\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       4/49      1.12G    0.05203    0.01151   0.007541         10        320: 100% 67/67 [00:56<00:00,  1.18it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.48it/s]\n","                   all         59         84      0.505      0.497      0.503      0.239\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       5/49      1.12G    0.04605    0.01032   0.005977         12        320: 100% 67/67 [00:54<00:00,  1.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.06it/s]\n","                   all         59         84      0.633      0.579      0.632      0.363\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       6/49      1.12G    0.04381   0.009466   0.005193         11        320: 100% 67/67 [00:55<00:00,  1.21it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.79it/s]\n","                   all         59         84      0.556      0.718       0.67      0.337\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       7/49      1.12G     0.0413    0.00897    0.00452          7        320: 100% 67/67 [00:54<00:00,  1.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:01<00:00,  1.83it/s]\n","                   all         59         84      0.633      0.683      0.676      0.377\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       8/49      1.12G    0.03927   0.008375   0.004244          7        320: 100% 67/67 [00:55<00:00,  1.22it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.08it/s]\n","                   all         59         84      0.837      0.654      0.772      0.453\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","       9/49      1.12G    0.03768    0.00842   0.004049         11        320: 100% 67/67 [00:54<00:00,  1.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.65it/s]\n","                   all         59         84      0.691      0.806      0.766      0.402\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      10/49      1.12G    0.03569   0.008174   0.003676          5        320: 100% 67/67 [00:55<00:00,  1.20it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 2/2 [00:00<00:00,  2.34it/s]\n","                   all         59         84      0.746      0.784      0.783      0.465\n"]}],"source":["# Execute the training\n","!python yolov5/train.py --img 320 --batch 16 --epochs 50 --data TrafficVOC.yaml --weights yolov5s.pt --workers 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZJC2OYXoAbo"},"outputs":[],"source":["# Load the trained weights\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/runs/train/exp4/weights/best.pt', force_reload=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jqr_N8nrvI3"},"outputs":[],"source":["# PASAR UN VIDEO -> SUBIR EL VIDEO, DIVIDIRLO EN FOTOGRAMAS, Y PASARLE CADA FOTOGRAMA POR EL VIDEO\n","\n","def predict_video(video_path, output_path):\n","  cap = cv2.VideoCapture(video_path)\n","\n","  #OUTPUT VIDEO\n","  frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","  frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","  fps = cap.get(cv2.CAP_PROP_FPS)\n","  codec = cv2.VideoWriter_fourcc(*\"mp4v\")\n","  output_video = cv2.VideoWriter(output_path, codec, fps, (frame_width, frame_height))\n","\n","  while cap.isOpened():\n","      ret, frame = cap.read()\n","\n","      if not ret:\n","          break\n","\n","      # RESIZE IMAGE\n","      height, width = frame.shape[:2]\n","      frame = cv2.resize(frame, (2 * width, 2 * height))\n","\n","      result = model(frame)\n","\n","      output_video.write(np.squeeze(cv2.resize(np.squeeze(result.render()),(width, height))))\n","\n","  cap.release()\n","  output_video.release()\n","  cv2.destroyAllWindows()\n","\n","\n","model.conf = 0.25\n","predict_video(test_videos_path + '/TestVideo1.mp4', test_videos_path + '/TestVideo1_Yolov5_ultima_oportunidad.mp4')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIqry_1TqWSA"},"outputs":[],"source":["# OLD CODE TO EXECUTE THE TRAINING (LOWER LOSSES)\n","\n","# Load a model\n","#model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n","#model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n","#model = YOLO(\"yolov5s.yaml\")  # build a new model from scratch\n","#model = YOLO(\"yolov5su.pt\")  # load a pretrained model (recommended for training)\n","\n","#model.add_callback(\"on_train_start\", freeze_layer)\n","\n","#model.train(data=\"TrafficVOC.yaml\", epochs=50, imgsz = 640)  # train the model\n","#metrics = model.val()  # evaluate model performance on the validation set\n","\n","#model.export()\n","\n","#model = YOLO(\"runs/detect/train9/weights/best.pt\")\n","\n","#model.predict(test_videos_path+\"/TestVideo1.mp4\", save=True)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
